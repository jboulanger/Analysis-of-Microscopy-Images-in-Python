{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Distributed computing on HPC\n",
    "\n",
    "To scale up computation, it can be relevant to transfer the computational load\n",
    "to a remote server such as a high performance computing (HPC) cluster from a \n",
    "jupyter notebook.\n",
    "\n",
    "We need first to create an environment with the necessary packages on the cluster,\n",
    " or add those to an existing environment.\n",
    "\n",
    "## Installation\n",
    "1. Connect to the remote server with ssh from your local machine:\n",
    "```\n",
    "ssh <REMOTE_USER>@<REMOTE_HOST>\n",
    "```\n",
    "2. Install  miniconda\n",
    "```\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "./Miniconda3-latest-Linux-x86_64.sh\n",
    "conda init tcsh\n",
    "```\n",
    "3. Create a conda environement and install Jupyter Lab \n",
    "```\n",
    "conda create -n dasktest\n",
    "conda activate dasktest\n",
    "conda install jupyterlab nodejs ipywidgets -c conda-forge -y\n",
    "# optional packages:\n",
    "# conda install scikit-image matplotlib pandas -y\n",
    "```\n",
    "4. Register the jupyter kernel\n",
    "```\n",
    "python -m ipykernel install --user --name dasktest\n",
    "```\n",
    "\n",
    "Note if you have already a working environment (eg myenv), you only need to make \n",
    "sure that you have a jupyter lab package installed:\n",
    "```\n",
    "conda activate myenv\n",
    "conda install jupyterlab nodejs ipywidgets -c conda-forge -y\n",
    "python -m ipykernel install --user --name myenv\n",
    "\n",
    "```\n",
    "\n",
    "## Connecting to a Jupyter notebook running on a remote server\n",
    "We want to run a jupyter on the server from a local computer, to do so we need\n",
    "to configure an python environment on the remote server, then we can connect \n",
    "directly or via a ssh tunnel.\n",
    "\n",
    "### Directly\n",
    "1. Connect to the remote computer\n",
    "```\n",
    "ssh <REMOTE_USER>@<REMOTE_HOST>\n",
    "```\n",
    "2. Optinally request ressources on the cluster. In a SLURM managed cluster, type\n",
    "```\n",
    "srun --partition gpu --pty tcsh -i\n",
    "```\n",
    "2. Start jupyter on the remote computer\n",
    "```\n",
    "conda activate dasktest\n",
    "jupyter lab --no-browser --ip 0.0.0.0\n",
    "```\n",
    "3. Copy the link indicating the remote computer, click in the 'Jupyter Server:Local' in visual code menu bar and paste the link in the menu appearing at the top of the window. You can also open the link in a a browser.\n",
    "\n",
    "### With a ssh tunnel\n",
    "1. Open a ssh tunnel using the same port than the notebook by runing on the local\n",
    "machine:\n",
    "```\n",
    "ssh -L 8080:localhost:8080 <REMOTE_USER>@<REMOTE_HOST>\n",
    "```\n",
    "This command opend a interactive session on the cluster.\n",
    "2. Start a jupyter lab server from the environment\n",
    "```\n",
    "conda activate dasktest\n",
    "jupyter lab --no-browser --ip=\"*\" --port 8080\n",
    "```\n",
    "3. Connect to the notebook by opening a browser and navigating to\n",
    "http://localhost:8080 or use the link provided http://localhost:8888/lab?token=\n",
    "Alternatively, you can use a remote server in visual code by clicking on\n",
    "Jupyter Server:Local in the task bar and paste the link http://localhost:8888/lab?token=\n",
    "when prompted. New kernel will then be visible.\n",
    "\n",
    "At this point, we have an openned terminal connected to the cluster with\n",
    "jupyter lab running. We also have either a web browser tab or a Visual Code displaying a notebook.\n",
    "\n",
    "At  the end of the session, we need to stop Jupyter Lab by pressing CTRL-C in the\n",
    "terminal running Jupyter Lab or using the File>Shutdown in the jupyter lab \n",
    "interface. Then logout from the terminal to stop the session.\n",
    "\n",
    "## Using Dask distributed\n",
    "Dask allows to perform parallel and distributed in python using well know data\n",
    "structures such as numpy's ndarray and pandas's dataframes. Additionally we can \n",
    "use [dask-jobqueue](https://jobqueue.dask.org/) to manage the connection to a job \n",
    "scheduler such as SLURM.\n",
    "\n",
    "We need to install dask on the remote computer and the extensions for jupyter lab:\n",
    "```\n",
    "condata activate dasktest\n",
    "conda install dask distributed -c conda-forge\n",
    "pip install dask_labextension\n",
    "jupyter labextension install dask-labextension\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```\n",
    "\n",
    "Open a notebook on the remote computer and create a cluster scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, progress\n",
    "cluster = SLURMCluster(\n",
    "     cores=1,       # size of a single job\n",
    "     memory='64GB', # for a single job\n",
    "     shebang='#!/usr/bin/env tcsh',\n",
    "     processes=1,   # number of python process per job\n",
    "     queue='',      # cpu,gpu or ml\n",
    "     local_directory='/ssd',               \n",
    "     walltime='02:00:00', # 2 hours wall time\n",
    ")\n",
    "cluster.adapt(maximum_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a client to connect to the scheduler and display the client. This will\n",
    "print a link that you can copy paste in the Juypter lab dask extension tab in \n",
    "order to monitor the active processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One typical example is to load a list of files to process in a data frame. \n",
    "See also the [bath-processing](batch-processing.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "folder = Path('../data')\n",
    "# load a Dask Data Frame listing the files and additional informations\n",
    "exp = pd.read_csv(folder/'experiment.csv')\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dask, we can then map each entry to be processed in parallel.\n",
    "\n",
    "Note that calling dask.delayed on function loading array using dask will load \n",
    "the all file each time. Here we lazily load the images before hand and process\n",
    "them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2\n",
    "import dask\n",
    "\n",
    "# load all images lazily\n",
    "imgs = [nd2.imread(folder/f, dask=True) for f in exp['filename']]\n",
    "\n",
    "# process each image\n",
    "def process_image(img):\n",
    "    return img.mean(), img.std()    \n",
    "\n",
    "# create tasks for each file\n",
    "tsk = [dask.delayed(process_image)(img) for img in imgs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the cluster (ask for workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check now in the command line that opened the tunnel for example, the \n",
    "status of the worker using the command:\n",
    "```\n",
    "squeue -u $USER\n",
    "```\n",
    "Note, send the jupyter notebook server in the background using `Ctrl-z` in the \n",
    "terminal and then type the previous command. Bring back jupyter in the forground\n",
    "typing `fg`.\n",
    "\n",
    "Once the jobs are running (the column ST should display R when running squeue),\n",
    "you can start launch the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tasks\n",
    "result = dask.compute(tsk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to store the result in a pandas' data frame, it can be convenient to\n",
    " map a function to the input list of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# define the func to process blocks of the dataframe\n",
    "def process_rows(df):\n",
    "      \"\"\"Process rows of the data frame\"\"\"\n",
    "      result = []\n",
    "      for x in df.itertuples():            \n",
    "            # retreive the line of the input data frame\n",
    "            # for example we could open a file and process it\n",
    "            fname = x.filename\n",
    "            m = 1\n",
    "            # create a data frame, note that values must be lists or you need \n",
    "            # to pass an index            \n",
    "            result.append(pd.DataFrame({'filename':[fname], 'mean':[m]}))\n",
    "      return pd.concat(result,ignore_index=True)\n",
    "\n",
    "\n",
    "# schedule the computations\n",
    "ddf = dd.from_pandas(exp, chunksize=1).map_partitions(process_rows,\n",
    "                            meta={'filename':'object', 'mean':'f'})\n",
    "\n",
    "# compute the values\n",
    "res = ddf.compute()\n",
    "# merge the new columns to the original table\n",
    "exp.merge(res, on='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch process files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import dask\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "\n",
    "#Define the function to load and process files\n",
    "def process_file(filename):\n",
    "    img = tifffile.imread(filename)\n",
    "    return img.mean()\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# define the folder where the datafiles are\n",
    "folder = Path('../data/')\n",
    "\n",
    "# list all tif files\n",
    "filelist = [f for f in folder.glob('[!.]*.tif')]\n",
    "\n",
    "# create a task for each files, and start them immediately\n",
    "tsk = [client.submit(process_file,f) for f in filelist]\n",
    "\n",
    "# gather the results and store it in a data frame\n",
    "pd.DataFrame({\n",
    "    'File name':filelist,\n",
    "    'Mean intenisty':[r.result() for r in tsk]\n",
    "    })\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define two functions that we combine to define a graph of task that\n",
    "will be executed lazily. For this we can use the decorator `dask.delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import dask\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "\n",
    "#Define the function to load files\n",
    "@dask.delayed\n",
    "def load_file(filename):    \n",
    "    img = tifffile.imread(filename)\n",
    "    return img\n",
    "\n",
    "#Define the function to process the image\n",
    "@dask.delayed\n",
    "def process_image(img):            \n",
    "    return img.mean()\n",
    "\n",
    "# define the folder where the datafiles are\n",
    "folder = Path('../data/')\n",
    "\n",
    "# list all tif files\n",
    "filelist = [f for f in folder.glob('[!.]*.tif')]\n",
    "\n",
    "# create a list of task \n",
    "tsk = [process_image(load_file(f)) for f in filelist]\n",
    "\n",
    "# gather the results and store it in a data frame\n",
    "pd.DataFrame({\n",
    "    'File name':filelist,\n",
    "    'Mean intenisty':[r.compute() for r in tsk]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy loading\n",
    "\n",
    "How to read a tiff as a delayed dask array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import tifffile\n",
    "import dask.array\n",
    "\n",
    "def tiffileimreaddask(filename):\n",
    "    store = tifffile.imread(filename, aszarr=True)\n",
    "    array = dask.array.from_zarr(store)\n",
    "    return array\n",
    "\n",
    "img = tiffileimreaddask('../scratch/tmp.tif')\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('imaging')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "name": "Untitled.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "af652b78da32f40db052c887d212218f2b9dfc5bd9e07e878617985773e27cfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
